{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9253319,"sourceType":"datasetVersion","datasetId":5598328}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nfrom datasets import Dataset, DatasetDict\nfrom collections import defaultdict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Segment Text by Emotion\ndef segment_text_by_emotion(df):\n    emotion_segments = defaultdict(list)\n    current_emotion = None\n    current_segment = []\n\n    for _, row in df.iterrows():\n        utterance = row['Utterance']\n        emotion = row['Emotion']\n\n        if current_emotion is None:\n            current_emotion = emotion\n\n        if emotion != current_emotion:\n            emotion_segments[current_emotion].append(' '.join(current_segment))\n            current_emotion = emotion\n            current_segment = []\n\n        current_segment.append(utterance)\n\n    if current_segment:\n        emotion_segments[current_emotion].append(' '.join(current_segment))\n\n    return emotion_segments","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Prepare the Data for Fine-Tuning\ndef prepare_dataset(emotion_segments):\n    data = {\n        \"input_text\": [],\n        \"summary_text\": [],\n    }\n\n    for emotion, segments in emotion_segments.items():\n        for segment in segments:\n            summary = f\"This segment expresses {emotion}.\"\n            data[\"input_text\"].append(segment)\n            data[\"summary_text\"].append(summary)\n\n    dataset = Dataset.from_dict(data)\n    return DatasetDict({\"train\": dataset, \"validation\": dataset})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Fine-Tune BART Model\ndef fine_tune_bart(train_dataset, val_dataset, output_dir=\"./bart-emotion-summarization\"):\n    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n\n    def tokenize_function(examples):\n        model_inputs = tokenizer(examples[\"input_text\"], max_length=1024, padding=\"max_length\", truncation=True)\n        labels = tokenizer(examples[\"summary_text\"], max_length=150, padding=\"max_length\", truncation=True)\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    tokenized_train = train_dataset.map(tokenize_function, batched=True)\n    tokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        eval_strategy=\"epoch\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=4,\n        per_device_eval_batch_size=4,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        save_total_limit=2,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        data_collator=data_collator,\n    )\n\n    trainer.train()\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n# Main Execution Flow\nif __name__ == \"__main__\":\n    df = pd.read_csv(\"/kaggle/input/train-sent-emo-csv/train_sent_emo.csv\")\n    emotion_segments = segment_text_by_emotion(df)\n    dataset = prepare_dataset(emotion_segments)\n    fine_tune_bart(dataset[\"train\"], dataset[\"validation\"])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-27T10:11:04.795486Z","iopub.execute_input":"2024-08-27T10:11:04.796139Z","iopub.status.idle":"2024-08-27T12:15:18.433941Z","shell.execute_reply.started":"2024-08-27T10:11:04.796098Z","shell.execute_reply":"2024-08-27T12:15:18.432776Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2b040556e7411880aaf439f7340998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f159f23d43ab4959a367eb5a87769642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c086ea4c202411a8b54120846f4b86e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6080931d59c04e67a7138242073112c5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb772971e634c478fc59ff2600a33b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0da657d2b4c40a397436b01e08e59ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6096 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18faf8c7b5cb416ab94041905c0b5063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6096 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83d58e048f54b34b8cd47ae2a8dc56e"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240827_101231-6td8i0hi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface/runs/6td8i0hi' target=\"_blank\">./bart-emotion-summarization</a></strong> to <a href='https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface' target=\"_blank\">https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface/runs/6td8i0hi' target=\"_blank\">https://wandb.ai/priyansha-upadhyay1304-christ-university/huggingface/runs/6td8i0hi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4572' max='4572' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4572/4572 2:02:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.009600</td>\n      <td>0.057192</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.006500</td>\n      <td>0.047271</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.004100</td>\n      <td>0.032401</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls ./bart-emotion-summarization","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:28:40.343137Z","iopub.execute_input":"2024-08-27T12:28:40.343585Z","iopub.status.idle":"2024-08-27T12:28:41.445939Z","shell.execute_reply.started":"2024-08-27T12:28:40.343546Z","shell.execute_reply":"2024-08-27T12:28:41.444834Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"checkpoint-4500  generation_config.json  runs\t\t\t  vocab.json\ncheckpoint-4572  merges.txt\t\t special_tokens_map.json\nconfig.json\t model.safetensors\t tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\n\n# Zip the directory containing the saved model\nshutil.make_archive(\"bart-emotion-summarization\", 'zip', 'out')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:17:12.796547Z","iopub.execute_input":"2024-08-27T12:17:12.796940Z","iopub.status.idle":"2024-08-27T12:17:12.805927Z","shell.execute_reply.started":"2024-08-27T12:17:12.796904Z","shell.execute_reply":"2024-08-27T12:17:12.804986Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bart-emotion-summarization.zip'"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/bart-emotion-summarization')\n    zip_name = f\"/kaggle/working/bart-emotion-summarization{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n\n\ndownload_file('/kaggle/working/bart-emotion-summarization', 'out')","metadata":{"execution":{"iopub.status.busy":"2024-08-27T12:31:15.326286Z","iopub.execute_input":"2024-08-27T12:31:15.326840Z","iopub.status.idle":"2024-08-27T12:40:07.988678Z","shell.execute_reply.started":"2024-08-27T12:31:15.326798Z","shell.execute_reply":"2024-08-27T12:40:07.987654Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Unable to run zip command!\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}